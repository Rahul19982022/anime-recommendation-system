{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7HHNngjX+k7o7NdLhMiLh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO2ezfUojySa",
        "outputId": "0090e2fd-e825-41f1-d0f1-5523d54a9daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Setup complete. Working directory: /content/drive/MyDrive/anime_recommendation_system\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Libraries & Setup Environment\n",
        "!pip install -q streamlit pyngrok sentence-transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import gc\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import tensorflow\n",
        "import tensorflow.keras\n",
        "from numpy.linalg import norm\n",
        "from google.colab import drive\n",
        "import requests\n",
        "import time\n",
        "import networkx as nx\n",
        "\n",
        "# Mount Drive and set path\n",
        "drive.mount('/content/drive')\n",
        "proj_path = '/content/drive/MyDrive/anime_recommendation_system'\n",
        "os.chdir(proj_path)\n",
        "print(f\"‚úÖ Setup complete. Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: One-Time Data Preparation (Now includes Synopsis)\n",
        "print(\"Starting one-time data preparation...\")\n",
        "\n",
        "# --- 0. CREATE NEW FOLDER ---\n",
        "data_save_path = 'app_data'\n",
        "os.makedirs(data_save_path, exist_ok=True)\n",
        "print(f\"...Created new data folder at: {data_save_path}\")\n",
        "\n",
        "# --- 1. Load Initial Data ---\n",
        "user = pd.read_pickle('datasets/created_datasets/user.pkl')\n",
        "anime = pd.read_pickle('datasets/created_datasets/anime.pkl')\n",
        "print(\"...Initial data loaded.\")\n",
        "\n",
        "# --- 2. Run Preprocessing ---\n",
        "anime['Genres_edited'] = anime['Genres'].copy()\n",
        "anime['Genres_edited'] = anime['Genres_edited'].str.replace('Slice of Life', 'Slice-of-Life')\n",
        "anime['Genres_edited'] = anime['Genres_edited'].str.replace('Martial Arts', 'Martial-Arts')\n",
        "anime['Genres_edited'] = anime['Genres_edited'].str.replace('Super Power', 'Super-Power')\n",
        "\n",
        "def space_split_genres(genres):\n",
        "    genres_edited = genres.copy()\n",
        "    for i in range(len(genres)):\n",
        "        if ' ' in genres[i]:\n",
        "            genres_edited = genres_edited[0:i] + genres[i].split(' ') + genres[i+1:]\n",
        "    return ('|').join(sorted(genres_edited))\n",
        "anime['Genres_edited'] = anime['Genres_edited'].str.split(', ').apply(space_split_genres)\n",
        "\n",
        "def year_finding(aired):\n",
        "    if aired == 'Unknown': return None\n",
        "    match = re.search(r'([1-2][0-9]{3})', aired)\n",
        "    if match is not None: return int(match.group(0))\n",
        "    else: return -1\n",
        "\n",
        "anime['Origin_year'] = anime['Aired'].apply(lambda x: year_finding(str(x))).astype('Int32')\n",
        "# Only one anime was found to be having None as value, 'Katsudou Shashin' is the name, manually I found its Origin Year.\n",
        "anime.loc[anime['MAL_ID'] == 33187, 'Origin_year'] = 1907\n",
        "print(\"...Base anime processing complete.\")\n",
        "\n",
        "# --- 3. Load and Merge Synopsis Data ---\n",
        "try:\n",
        "    synopsis_df = pd.read_csv('datasets/kaggle_dataset/anime_with_synopsis.csv')\n",
        "    synopsis_df.rename(columns={'sypnopsis': 'synopsis'}, inplace=True)\n",
        "    # Keep only the columns we need to merge\n",
        "    synopsis_df = synopsis_df[['MAL_ID', 'synopsis']]\n",
        "\n",
        "    # Merge with our processed anime dataframe\n",
        "    anime = anime.merge(synopsis_df, on='MAL_ID', how='left')\n",
        "    print(\"...Synopsis data loaded and merged.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"...Synopsis file not found. Skipping synopsis merge.\")\n",
        "    anime['synopsis'] = \"No synopsis available.\"\n",
        "except Exception as e:\n",
        "    print(f\"...Error merging synopsis: {e}. Skipping.\")\n",
        "    anime['synopsis'] = \"No synopsis available.\"\n",
        "\n",
        "# --- 4. Create and Save Anime Aggregates ---\n",
        "anime_agg = user.groupby('anime_id')['rating'].agg(['mean', 'count']).reset_index()\n",
        "anime_agg.columns = ['anime_id', 'anime_avg_rating', 'total_ratings_anime']\n",
        "anime_agg.to_pickle(os.path.join(data_save_path, 'anime_agg_processed.pkl'))\n",
        "\n",
        "# Building Popular_adjusted (Popularity column become continuous and have non-repetitive entries)\n",
        "anime['total_ratings_anime'] = anime['MAL_ID'].map(anime_agg.set_index('anime_id')['total_ratings_anime'])\n",
        "sorted_indices = anime.sort_values(by=['Popularity', 'total_ratings_anime'], ascending=[True, False]).index\n",
        "anime.loc[sorted_indices, 'Popularity_adjusted'] = range(1, len(anime) + 1)\n",
        "anime['Popularity_adjusted'] = anime['Popularity_adjusted'].astype('int')\n",
        "anime.drop(columns=['total_ratings_anime'], inplace=True)\n",
        "\n",
        "# --- 5. Save the processed dataframe ---\n",
        "anime.to_pickle(os.path.join(data_save_path, 'anime_processed.pkl'))\n",
        "print(\"...Processed 'anime_processed.pkl' (with synopsis) saved.\")\n",
        "\n",
        "# --- 6. Create and Save Genre MLB File ---\n",
        "genre_value_counts = pd.Series(('|').join(anime['Genres_edited'].fillna('')).split('|')).value_counts()\n",
        "genres_list = genre_value_counts.index.tolist()\n",
        "if 'Unknown' in genres_list: genres_list.remove('Unknown')\n",
        "genres_mlb_obj = MultiLabelBinarizer(classes=genres_list)\n",
        "anime_genres_mlb_df = anime[['MAL_ID', 'Genres_edited']].copy()\n",
        "anime_genres_mlb_temp = pd.DataFrame(\n",
        "    genres_mlb_obj.fit_transform(anime_genres_mlb_df['Genres_edited'].fillna('').str.split('|')),\n",
        "    columns=genres_mlb_obj.classes_,\n",
        "    index=anime_genres_mlb_df.index\n",
        ")\n",
        "anime_genres_mlb_df = anime_genres_mlb_df.join(anime_genres_mlb_temp)\n",
        "anime_genres_mlb_df.to_pickle(os.path.join(data_save_path, 'anime_genres_mlb.pkl'))\n",
        "with open(os.path.join(data_save_path, 'genres_list.pkl'), 'wb') as f:\n",
        "    pickle.dump(genres_list, f)\n",
        "print(\"...Genre MLB file and genres list saved.\")\n",
        "\n",
        "# --- 7. Create and Save Divided Opinion IDs ---\n",
        "anime_rating_freq = user.groupby(['anime_id','rating'])['rating'].count().unstack(fill_value = 0)\n",
        "anime_rating_freq.columns = [str(col) for col in anime_rating_freq.columns]\n",
        "anime_rating_freq = anime_rating_freq.add_prefix('rating_')\n",
        "rating_columns = anime_rating_freq.columns\n",
        "bad_rating_columns  = [f'rating_{i}' for i in range(1, 6) if f'rating_{i}' in rating_columns]\n",
        "good_rating_columns = [f'rating_{i}' for i in range(8, 11) if f'rating_{i}' in rating_columns]\n",
        "total_ratings = anime_rating_freq[rating_columns].sum(axis=1)\n",
        "anime_rating_freq['bad_rating_ratio'] = anime_rating_freq[bad_rating_columns].sum(axis=1) / total_ratings\n",
        "anime_rating_freq['good_rating_ratio'] = anime_rating_freq[good_rating_columns].sum(axis=1) / total_ratings\n",
        "anime_rating_freq['bad_to_good_ratio'] = anime_rating_freq['bad_rating_ratio'] / anime_rating_freq['good_rating_ratio']\n",
        "anime_rating_freq['bad_to_good_ratio'] = anime_rating_freq['bad_to_good_ratio'].replace([np.inf, -np.inf], np.nan)\n",
        "divided_opinion_cond = ((anime_rating_freq['bad_rating_ratio'] >= 0.25) & (anime_rating_freq['bad_to_good_ratio'].between(0.9, 1.15)))\n",
        "divided_opinion_animes_mal_ids = anime_rating_freq[divided_opinion_cond].index.values\n",
        "with open(os.path.join(data_save_path, 'divided_opinion_anime_ids.pkl'), 'wb') as f:\n",
        "    pickle.dump(divided_opinion_animes_mal_ids, f)\n",
        "print(f\"...Divided Opinion file saved. Found {len(divided_opinion_animes_mal_ids)} animes.\")\n",
        "\n",
        "\n",
        "# --- 8. NEW: EXTRACT AND SAVE MODEL WEIGHTS ---\n",
        "print(\"...Loading model to extract weights...\")\n",
        "model = tensorflow.keras.models.load_model('model/checkpoint.model.keras')\n",
        "try:\n",
        "    # Get weights from the layer\n",
        "    weights = model.get_layer('anime_embedding').get_weights()[0]\n",
        "    # Normalize them immediately to save computation time in the app\n",
        "    weights_norm = weights / norm(weights, axis=1).reshape((-1, 1))\n",
        "\n",
        "    # Save as a simple pickle file\n",
        "    with open(os.path.join(data_save_path, 'anime_model_weights.pkl'), 'wb') as f:\n",
        "        pickle.dump(weights_norm, f)\n",
        "    print(f\"Weights extracted! Saved 'anime_model_weights.pkl'. The big model file is NOT needed for the app.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting weights: {e}\")\n",
        "\n",
        "# --- 9. Clean up memory ---\n",
        "del user\n",
        "del anime_rating_freq\n",
        "del anime\n",
        "del anime_agg\n",
        "del model\n",
        "del sorted_indices\n",
        "del anime_genres_mlb_temp\n",
        "del anime_genres_mlb_df\n",
        "del divided_opinion_cond\n",
        "del divided_opinion_animes_mal_ids\n",
        "del genre_value_counts\n",
        "del genres_mlb_obj\n",
        "del weights\n",
        "del weights_norm\n",
        "del bad_rating_columns\n",
        "del good_rating_columns\n",
        "del total_ratings\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n‚úÖ All data files for the DEPLOYABLE app have been created in 'app_data/'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJraK_yKj58D",
        "outputId": "37f7ed2b-05ed-40d3-aa5b-80e9161c677a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting one-time data preparation...\n",
            "...Created new data folder at: app_data\n",
            "...Initial data loaded.\n",
            "...Base anime processing complete.\n",
            "...Synopsis data loaded and merged.\n",
            "...Processed 'anime_processed.pkl' (with synopsis) saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Unknown'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Genre MLB file and genres list saved.\n",
            "...Divided Opinion file saved. Found 208 animes.\n",
            "...Loading model to extract weights...\n",
            "Weights extracted! Saved 'anime_model_weights.pkl'. The big model file is NOT needed for the app.\n",
            "\n",
            "‚úÖ All data files for the DEPLOYABLE app have been created in 'app_data/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data Enrichment (Fetch Posters)\n",
        "\n",
        "print(\"Starting data enrichment\")\n",
        "\n",
        "# Define the path to our data\n",
        "data_path = 'app_data'\n",
        "anime_processed_path = os.path.join(data_path, 'anime_processed.pkl')\n",
        "anime_with_posters_path = os.path.join(data_path, 'anime_with_posters.pkl')\n",
        "\n",
        "# Load the dataframe we just created\n",
        "anime = pd.read_pickle(anime_processed_path)\n",
        "\n",
        "# Create a new column for the image URL if it doesn't exist\n",
        "if 'image_url' not in anime.columns:\n",
        "    anime['image_url'] = None\n",
        "\n",
        "# Define the Jikan API endpoint\n",
        "JIKAN_API_URL = \"https://api.jikan.moe/v4/anime/{}/pictures\"\n",
        "\n",
        "i = 0\n",
        "# --- Loop through each anime to get its poster URL ---\n",
        "for index, row in anime.iterrows():\n",
        "\n",
        "    i += 1\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed {i} animes\")\n",
        "    mal_id = row['MAL_ID']\n",
        "\n",
        "    # Only fetch if we don't already have the URL\n",
        "    if pd.isna(row['image_url']):\n",
        "        try:\n",
        "            # 1. Call the API\n",
        "            response = requests.get(JIKAN_API_URL.format(mal_id))\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                # 2. Get the poster URL from the JSON\n",
        "                data = response.json()\n",
        "                if data['data']: # Check if data is not empty\n",
        "                    poster_url = data['data'][0]['jpg']['image_url']\n",
        "                    anime.at[index, 'image_url'] = poster_url\n",
        "                else:\n",
        "                    anime.at[index, 'image_url'] = \"NOT_FOUND\"\n",
        "\n",
        "            else:\n",
        "                anime.at[index, 'image_url'] = \"NOT_FOUND\"\n",
        "\n",
        "            time.sleep(1) # 1 request per second to avoid rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"üö® Error on ID {mal_id}: {e}\")\n",
        "            anime.at[index, 'image_url'] = \"NOT_FOUND\"\n",
        "            time.sleep(5) # Wait longer if there's an error\n",
        "\n",
        "# --- Save the final, enriched dataframe ---\n",
        "anime.to_pickle(anime_with_posters_path)\n",
        "\n",
        "print(f\"\\n--- ENRICHMENT COMPLETE! ---\")\n",
        "print(f\"‚úÖ Successfully saved enriched data to '{anime_with_posters_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2FDCgrpj5-3",
        "outputId": "00488197-125a-4973-95ec-ef8a43966726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data enrichment\n",
            "Processed 100 animes\n",
            "Processed 200 animes\n",
            "Processed 300 animes\n",
            "Processed 400 animes\n",
            "Processed 500 animes\n",
            "Processed 600 animes\n",
            "Processed 700 animes\n",
            "Processed 800 animes\n",
            "Processed 900 animes\n",
            "Processed 1000 animes\n",
            "Processed 1100 animes\n",
            "Processed 1200 animes\n",
            "Processed 1300 animes\n",
            "Processed 1400 animes\n",
            "Processed 1500 animes\n",
            "Processed 1600 animes\n",
            "Processed 1700 animes\n",
            "Processed 1800 animes\n",
            "Processed 1900 animes\n",
            "Processed 2000 animes\n",
            "Processed 2100 animes\n",
            "Processed 2200 animes\n",
            "Processed 2300 animes\n",
            "Processed 2400 animes\n",
            "Processed 2500 animes\n",
            "Processed 2600 animes\n",
            "Processed 2700 animes\n",
            "Processed 2800 animes\n",
            "Processed 2900 animes\n",
            "Processed 3000 animes\n",
            "Processed 3100 animes\n",
            "Processed 3200 animes\n",
            "Processed 3300 animes\n",
            "Processed 3400 animes\n",
            "Processed 3500 animes\n",
            "Processed 3600 animes\n",
            "Processed 3700 animes\n",
            "Processed 3800 animes\n",
            "Processed 3900 animes\n",
            "Processed 4000 animes\n",
            "Processed 4100 animes\n",
            "Processed 4200 animes\n",
            "Processed 4300 animes\n",
            "Processed 4400 animes\n",
            "Processed 4500 animes\n",
            "Processed 4600 animes\n",
            "Processed 4700 animes\n",
            "Processed 4800 animes\n",
            "Processed 4900 animes\n",
            "Processed 5000 animes\n",
            "Processed 5100 animes\n",
            "Processed 5200 animes\n",
            "Processed 5300 animes\n",
            "Processed 5400 animes\n",
            "Processed 5500 animes\n",
            "Processed 5600 animes\n",
            "Processed 5700 animes\n",
            "Processed 5800 animes\n",
            "Processed 5900 animes\n",
            "Processed 6000 animes\n",
            "Processed 6100 animes\n",
            "Processed 6200 animes\n",
            "Processed 6300 animes\n",
            "\n",
            "--- ENRICHMENT COMPLETE! ---\n",
            "‚úÖ Successfully saved enriched data to 'app_data/anime_with_posters.pkl'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3b: Data Enrichment (Fetch Relations)\n",
        "\n",
        "\n",
        "print(\"Starting relationship mapping\")\n",
        "\n",
        "# --- 1. SETUP PATHS ---\n",
        "DATA_PATH = 'app_data'\n",
        "anime_with_posters_path = os.path.join(DATA_PATH, 'anime_with_posters.pkl')\n",
        "relations_save_path = os.path.join(DATA_PATH, 'anime_relations.pkl')\n",
        "\n",
        "# --- 2. LOAD DATA ---\n",
        "anime = pd.read_pickle(anime_with_posters_path)\n",
        "\n",
        "# --- 3. INIT DICTIONARY ---\n",
        "anime_relations = {}\n",
        "\n",
        "# --- 4. CONFIGURATION ---\n",
        "JIKAN_RELATIONS_URL = \"https://api.jikan.moe/v4/anime/{}/relations\"\n",
        "\n",
        "# Anime Related types to ignore, cause weak/no relations at all to anime story\n",
        "EXCLUDED_RELATION_TYPES = ['Other', 'Character']\n",
        "\n",
        "for index, row in anime.iterrows():\n",
        "\n",
        "    mal_id = row['MAL_ID']\n",
        "\n",
        "    try:\n",
        "        response = requests.get(JIKAN_RELATIONS_URL.format(mal_id))\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json().get('data', [])\n",
        "\n",
        "            related_ids = []\n",
        "            if data:\n",
        "                for relation_group in data:\n",
        "                    relation_type = relation_group.get('relation')\n",
        "\n",
        "                    # --- FILTER 1: Skip 'Other' and 'Character' relations ---\n",
        "                    if relation_type in EXCLUDED_RELATION_TYPES:\n",
        "                        continue\n",
        "\n",
        "                    for entry in relation_group['entry']:\n",
        "                        # --- FILTER 2: Skip Manga/Light Novels ---\n",
        "                        if entry['type'] == 'anime':\n",
        "                            related_ids.append(entry['mal_id'])\n",
        "\n",
        "            anime_relations[mal_id] = related_ids\n",
        "\n",
        "        else:\n",
        "            anime_relations[mal_id] = []\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üö® Exception for {mal_id}: {e}\")\n",
        "        time.sleep(5)\n",
        "\n",
        "# --- 5. SAVE ---\n",
        "with open(relations_save_path, 'wb') as f:\n",
        "    pickle.dump(anime_relations, f)"
      ],
      "metadata": {
        "id": "tvrNTxqR-dP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3c: Expand Relationships\n",
        "\n",
        "print(\"Starting relationship expansion...\")\n",
        "\n",
        "# 1. Setup Paths\n",
        "DATA_PATH = 'app_data'\n",
        "original_relations_path = os.path.join(DATA_PATH, 'anime_relations.pkl')\n",
        "expanded_relations_path = os.path.join(DATA_PATH, 'anime_relations_expanded.pkl')\n",
        "anime_processed_path = os.path.join(DATA_PATH, 'anime_with_posters.pkl') # Need this for validation\n",
        "\n",
        "# 2. Load Data\n",
        "with open(original_relations_path, 'rb') as f:\n",
        "    raw_relations = pickle.load(f)\n",
        "\n",
        "# Load our actual anime list to validate IDs\n",
        "anime_df = pd.read_pickle(anime_processed_path)\n",
        "valid_anime_ids = set(anime_df['MAL_ID'].unique())\n",
        "\n",
        "# Necessary data updation\n",
        "if 6115 in raw_relations[1412]:\n",
        "    raw_relations[1412].remove(6115)\n",
        "\n",
        "# 3. Build the Graph (With Validation)\n",
        "G = nx.Graph()\n",
        "\n",
        "for anime_id, related_ids in raw_relations.items():\n",
        "    # Only add the source node if it's in our dataset\n",
        "    if anime_id in valid_anime_ids:\n",
        "        G.add_node(anime_id)\n",
        "\n",
        "        for rel_id in related_ids:\n",
        "            if rel_id in valid_anime_ids:\n",
        "                G.add_edge(anime_id, rel_id)\n",
        "\n",
        "print(f\"Graph built. Found {G.number_of_nodes()} valid connected anime.\")\n",
        "\n",
        "# 4. Extract Families\n",
        "expanded_relations = {}\n",
        "for family in nx.connected_components(G):\n",
        "    family_list = list(family)\n",
        "    for anime_id in family_list:\n",
        "        expanded_relations[anime_id] = family_list\n",
        "\n",
        "# 5. Save\n",
        "with open(expanded_relations_path, 'wb') as f:\n",
        "    pickle.dump(expanded_relations, f)\n",
        "\n",
        "print(\"‚úÖ Expansion Complete! Data is now clean and strictly anime-only.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAQ7tvzRQQj8",
        "outputId": "73a2ffb4-bc1f-4ba1-ab7f-116c6072260e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting relationship expansion...\n",
            "Graph built. Found 6356 valid connected anime.\n",
            "‚úÖ Expansion Complete! Data is now clean and strictly anime-only.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create the 'recommender.py' Backend\n",
        "%%writefile streamlit_app/recommender.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sentence_transformers import util\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# --- 1. LOAD ALL FILES ---\n",
        "try:\n",
        "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "    PROJ_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))\n",
        "    DATA_PATH = os.path.join(PROJ_ROOT, 'app_data')\n",
        "\n",
        "    # Load DataFrames\n",
        "    anime = pd.read_pickle(os.path.join(DATA_PATH, 'anime_with_posters.pkl'))\n",
        "    anime_agg = pd.read_pickle(os.path.join(DATA_PATH, 'anime_agg_processed.pkl'))\n",
        "    content_df = pd.read_pickle(os.path.join(PROJ_ROOT, 'datasets/created_datasets/content_df_model.pkl'))\n",
        "    anime_genres_mlb = pd.read_pickle(os.path.join(DATA_PATH, 'anime_genres_mlb.pkl'))\n",
        "\n",
        "    # Load IDs and Lists\n",
        "    with open(os.path.join(DATA_PATH, 'divided_opinion_anime_ids.pkl'), 'rb') as f:\n",
        "        divided_opinion_ids = pickle.load(f)\n",
        "    with open(os.path.join(DATA_PATH, 'genres_list.pkl'), 'rb') as f:\n",
        "        genres_list = pickle.load(f)\n",
        "\n",
        "    # --- NEW: Load Relations Dictionary ---\n",
        "    with open(os.path.join(DATA_PATH, 'anime_relations_expanded.pkl'), 'rb') as f:\n",
        "        anime_relations = pickle.load(f)\n",
        "\n",
        "    # Filter content_df\n",
        "    content_df = content_df[content_df['MAL_ID'].isin(anime['MAL_ID'].unique())]\n",
        "\n",
        "    # Load Weights\n",
        "    with open(os.path.join(DATA_PATH, 'anime_model_weights.pkl'), 'rb') as f:\n",
        "        anime_weights = pickle.load(f)\n",
        "\n",
        "    # Load Encodings\n",
        "    with open(os.path.join(PROJ_ROOT, 'datasets/created_datasets/encoded_dictionary/anime2anime_encoded.pkl'), 'rb') as f:\n",
        "        anime2anime_encoded = pickle.load(f)\n",
        "    with open(os.path.join(PROJ_ROOT, 'datasets/created_datasets/encoded_dictionary/anime_encoded2anime.pkl'), 'rb') as f:\n",
        "        anime_encoded2anime = pickle.load(f)\n",
        "    encoded_dictionary = {'anime2anime_encoded': anime2anime_encoded, 'anime_encoded2anime': anime_encoded2anime}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading files: {e}\")\n",
        "    anime = pd.DataFrame()\n",
        "    anime_weights = None\n",
        "    anime_relations = {}\n",
        "\n",
        "# --- 2. CORE LOGIC FUNCTIONS ---\n",
        "def get_anime_id_from_name(name, anime_df):\n",
        "    try: return anime_df[anime_df['Name'].str.lower() == name.lower()]['MAL_ID'].values[0]\n",
        "    except:\n",
        "        try: return anime_df[anime_df['English name'].str.lower() == name.lower()]['MAL_ID'].values[0]\n",
        "        except: return None\n",
        "\n",
        "def get_anime_details(name, anime_df, anime_agg_df):\n",
        "    anime_id = get_anime_id_from_name(name, anime_df)\n",
        "    if anime_id is None: return None\n",
        "    anime_info = anime_df[anime_df['MAL_ID'] == anime_id].copy()\n",
        "    anime_info = anime_info.merge(anime_agg_df[['anime_id', 'anime_avg_rating']], left_on='MAL_ID', right_on='anime_id', how='left')\n",
        "    if anime_info.empty: return None\n",
        "    return anime_info\n",
        "\n",
        "# --- 3. FILTER FUNCTION ---\n",
        "def filter_recommendations(recommendations_df, anime_df, anime_agg_df, **filters):\n",
        "    if recommendations_df is None or recommendations_df.empty:\n",
        "        return recommendations_df\n",
        "\n",
        "    if 'MAL_ID' not in recommendations_df.columns:\n",
        "         recommendations_df.rename(columns={'anime_id': 'MAL_ID'}, inplace=True)\n",
        "\n",
        "    needed_info_cols = ['Name', 'English name', 'Type', 'Genres_edited', 'Origin_year', 'Popularity_adjusted', 'image_url', 'synopsis']\n",
        "    current_cols = set(recommendations_df.columns)\n",
        "    missing_cols = set(needed_info_cols) - current_cols\n",
        "    rec_with_info = recommendations_df.copy()\n",
        "\n",
        "    if len(missing_cols) > 0:\n",
        "        cols_to_merge = list(missing_cols) + ['MAL_ID']\n",
        "        rec_with_info = rec_with_info.merge(anime_df[cols_to_merge], on='MAL_ID', how='left')\n",
        "\n",
        "    rec_with_info = rec_with_info.merge(anime_agg_df[['anime_id', 'anime_avg_rating']], left_on='MAL_ID', right_on='anime_id', how='left')\n",
        "\n",
        "    if filters.get('Type_preferred'):\n",
        "        rec_with_info = rec_with_info[rec_with_info['Type'].isin(filters['Type_preferred'])]\n",
        "\n",
        "    if filters.get('Genres_preferred'):\n",
        "        genres_set = set(filters['Genres_preferred'])\n",
        "        rec_with_info = rec_with_info[rec_with_info['Genres_edited'].fillna('').str.split('|').apply(lambda x: genres_set.issubset(x))]\n",
        "\n",
        "    if filters.get('Origin_year_range'):\n",
        "        year_range = filters['Origin_year_range']\n",
        "        rec_with_info = rec_with_info[(rec_with_info['Origin_year'] >= year_range[0]) & (rec_with_info['Origin_year'] <= year_range[1])]\n",
        "\n",
        "    if filters.get('min_anime_rating'):\n",
        "        rec_with_info['anime_avg_rating'] = pd.to_numeric(rec_with_info['anime_avg_rating'], errors='coerce')\n",
        "        rec_with_info = rec_with_info.dropna(subset=['anime_avg_rating'])\n",
        "        rec_with_info = rec_with_info[rec_with_info['anime_avg_rating'] >= filters['min_anime_rating']]\n",
        "\n",
        "    if filters.get('popularity_range'):\n",
        "        pop_range = filters['popularity_range']\n",
        "        rec_with_info = rec_with_info[\n",
        "            (rec_with_info['Popularity_adjusted'] >= pop_range[0]) &\n",
        "            (rec_with_info['Popularity_adjusted'] <= pop_range[1])\n",
        "        ]\n",
        "\n",
        "    return rec_with_info.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# --- 4. GETTER FUNCTIONS ---\n",
        "def model_rec_based_on_anime_similarity(name, anime_df, weights, enc_dict):\n",
        "    if weights is None: return None\n",
        "    anime_id = get_anime_id_from_name(name, anime_df)\n",
        "    if anime_id is None: return None\n",
        "    encoded_index = enc_dict['anime2anime_encoded'].get(anime_id)\n",
        "    if encoded_index is None: return None\n",
        "\n",
        "    dists = np.dot(weights, weights[encoded_index])\n",
        "\n",
        "    dists_df = pd.DataFrame(dists, columns=['similarity_model'])\n",
        "    dists_df['MAL_ID'] = dists_df.index.map(enc_dict['anime_encoded2anime'])\n",
        "    dists_df = dists_df[dists_df['MAL_ID'] != anime_id]\n",
        "    return dists_df.sort_values(by='similarity_model', ascending=False)\n",
        "\n",
        "def content_based_rec(name, anime_df, content_df):\n",
        "    anime_id = get_anime_id_from_name(name, anime_df)\n",
        "    if anime_id is None: return None\n",
        "    try:\n",
        "        user_anime_vector = content_df.loc[content_df['MAL_ID'] == anime_id, 'plot_embeddings'].values[0]\n",
        "        scores = util.pytorch_cos_sim(user_anime_vector, np.array(content_df['plot_embeddings'].tolist()))[0]\n",
        "        results = pd.DataFrame({'MAL_ID': content_df['MAL_ID'], 'similarity': scores.numpy()})\n",
        "        results = results[results['MAL_ID'] != anime_id]\n",
        "        return results.sort_values(by='similarity', ascending=False)\n",
        "    except (IndexError, KeyError): return None\n",
        "\n",
        "def rec_based_on_genre_similarity(name, anime_df, anime_genre_mlb_df):\n",
        "    anime_id = get_anime_id_from_name(name, anime_df)\n",
        "    if anime_id is None: return None\n",
        "    try:\n",
        "        if 'MAL_ID' not in anime_genre_mlb_df.columns: return None\n",
        "        genre_columns = anime_genre_mlb_df.columns[2:]\n",
        "        selected_anime_genre_vector = anime_genre_mlb_df.loc[anime_genre_mlb_df['MAL_ID'] == anime_id, genre_columns].values.reshape(-1, 1)\n",
        "        anime_genre_array = anime_genre_mlb_df.loc[:, genre_columns].values\n",
        "\n",
        "        similarity_scores = np.dot(anime_genre_array, selected_anime_genre_vector).reshape(-1)\n",
        "        norm_factor = norm(anime_genre_array, axis=1) * norm(selected_anime_genre_vector)\n",
        "        norm_factor = np.where(norm_factor == 0, 1e-6, norm_factor)\n",
        "        similarity_scores = similarity_scores / norm_factor\n",
        "\n",
        "        results = pd.DataFrame({'MAL_ID': anime_genre_mlb_df['MAL_ID'], 'similarity_genre': similarity_scores})\n",
        "        results = results[results['MAL_ID'] != anime_id]\n",
        "        return results.sort_values(by='similarity_genre', ascending=False)\n",
        "    except (IndexError, KeyError): return None\n",
        "\n",
        "def rec_based_on_comb_of_genre_sim_and_model(name, anime_df, weights, enc_dict, genre_mlb_df, threshold=0.5):\n",
        "    if weights is None: return None\n",
        "    rec_model = model_rec_based_on_anime_similarity(name, anime_df, weights, enc_dict)\n",
        "    rec_genre = rec_based_on_genre_similarity(name, anime_df, genre_mlb_df)\n",
        "    if rec_model is None or rec_genre is None: return None\n",
        "    comb_rec = pd.merge(rec_genre, rec_model, on='MAL_ID', how='inner')\n",
        "    comb_rec = comb_rec[comb_rec['similarity_genre'] >= threshold]\n",
        "    return comb_rec.sort_values(by='similarity_model', ascending=False)\n",
        "\n",
        "def get_divided_opinion_animes(anime_df, ids_list):\n",
        "    if anime_df.empty: return pd.DataFrame()\n",
        "    divided_df = anime_df[anime_df['MAL_ID'].isin(ids_list)]\n",
        "    return divided_df\n",
        "\n",
        "# --- 5. MAIN ORCHESTRATOR FUNCTIONS ---\n",
        "def get_recommendations_by_name(name, rec_type, top_n=10, remove_related=False, **filters):\n",
        "    if anime.empty or anime_weights is None: return pd.DataFrame()\n",
        "\n",
        "    results_df = None\n",
        "    if rec_type == 'Model-Based Similarity':\n",
        "        results_df = model_rec_based_on_anime_similarity(name, anime, anime_weights, encoded_dictionary)\n",
        "    elif rec_type == 'Content (Plot) Similarity':\n",
        "        results_df = content_based_rec(name, anime, content_df)\n",
        "    elif rec_type == 'Combined Model + Genre':\n",
        "        results_df = rec_based_on_comb_of_genre_sim_and_model(name, anime, anime_weights, encoded_dictionary, anime_genres_mlb, threshold=filters.get('genre_threshold', 0.5))\n",
        "\n",
        "    if results_df is None or results_df.empty: return None\n",
        "\n",
        "    # --- NEW: Filter out related anime if requested ---\n",
        "    if remove_related:\n",
        "        input_id = get_anime_id_from_name(name, anime)\n",
        "        # Check if we have relations data for this anime\n",
        "        if input_id in anime_relations:\n",
        "            related_ids = anime_relations[input_id]\n",
        "            # Ensure proper column name for filtering\n",
        "            if 'MAL_ID' not in results_df.columns and 'anime_id' in results_df.columns:\n",
        "                 results_df.rename(columns={'anime_id': 'MAL_ID'}, inplace=True)\n",
        "\n",
        "            # Remove any recommendation whose ID is in the related list\n",
        "            if 'MAL_ID' in results_df.columns:\n",
        "                results_df = results_df[~results_df['MAL_ID'].isin(related_ids)]\n",
        "    # ------------------------------------------------\n",
        "\n",
        "    filtered_results = filter_recommendations(results_df, anime, anime_agg, **filters)\n",
        "\n",
        "    return filtered_results.head(top_n)[['Name', 'Genres_edited', 'Type', 'Origin_year', 'anime_avg_rating', 'Popularity_adjusted', 'image_url', 'synopsis']]\n",
        "\n",
        "def get_discover_animes(top_n=20):\n",
        "    if anime.empty: return pd.DataFrame()\n",
        "    divided_df = get_divided_opinion_animes(anime, divided_opinion_ids)\n",
        "    divided_df = divided_df.merge(anime_agg[['anime_id', 'anime_avg_rating']], left_on='MAL_ID', right_on='anime_id', how='left')\n",
        "    if top_n > len(divided_df): top_n = len(divided_df)\n",
        "    return divided_df.sample(n=top_n)[['Name', 'Genres_edited', 'Type', 'Origin_year', 'anime_avg_rating', 'Popularity_adjusted', 'image_url', 'synopsis']]"
      ],
      "metadata": {
        "id": "zgiAjJZwj6BT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0246c627-5ba6-460c-909d-04289b32cbbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/recommender.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Create the FINAL 'app.py' Frontend (With Relation Filter)\n",
        "%%writefile streamlit_app/app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from recommender import (\n",
        "    get_recommendations_by_name,\n",
        "    get_discover_animes,\n",
        "    anime,\n",
        "    genres_list,\n",
        "    get_anime_id_from_name,\n",
        "    get_anime_details,\n",
        "    anime_agg\n",
        ")\n",
        "\n",
        "# --- 1. UI Configuration ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Anime Recommendation System\")\n",
        "\n",
        "# --- 2. CUSTOM CSS ---\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "/* --- FIX #1: GLOBAL LINK HIDER --- */\n",
        "[data-testid=\"stHeaderActionElements\"] {\n",
        "    display: none !important;\n",
        "}\n",
        "h1, h2, h3 {\n",
        "    pointer-events: none !important;\n",
        "    text-decoration: none !important;\n",
        "}\n",
        "h1 a, h2 a, h3 a {\n",
        "    display: none !important;\n",
        "}\n",
        "\n",
        "/* --- FIX #2: Rose Pink Buttons (Primary Actions Only) --- */\n",
        "[data-testid=\"stSidebarUserContent\"] button[kind=\"primary\"] {\n",
        "    background-color: #C21E56 !important; /* Rose Pink */\n",
        "    color: white !important;              /* White Text */\n",
        "    border: none !important;\n",
        "    font-weight: 900 !important;          /* Max Font Weight */\n",
        "    font-size: 16px !important;           /* Larger Text */\n",
        "    text-shadow: 0.3px 0px 0px black;\n",
        "    transition: 0.2s;\n",
        "\n",
        "    /* Allow wrapping for primary buttons */\n",
        "    white-space: normal !important;\n",
        "    height: auto !important;\n",
        "    padding-top: 0.5rem !important;\n",
        "    padding-bottom: 0.5rem !important;\n",
        "    line-height: 1.2 !important;\n",
        "}\n",
        "[data-testid=\"stSidebarUserContent\"] button[kind=\"primary\"]:hover {\n",
        "    background-color: #A01848 !important; /* Darker Rose on hover */\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "}\n",
        "[data-testid=\"stSidebarUserContent\"] button[kind=\"primary\"] svg {\n",
        "    fill: white !important;\n",
        "    color: white !important;\n",
        "    stroke: white !important;\n",
        "    stroke-width: 1px;\n",
        "}\n",
        "\n",
        "/* --- FIX #3: Secondary Button Styling --- */\n",
        "/* Ensure standard buttons (like popovers) handle text properly */\n",
        "[data-testid=\"stSidebarUserContent\"] button[kind=\"secondary\"] {\n",
        "    white-space: normal !important;\n",
        "    height: auto !important;\n",
        "    padding-top: 0.2rem !important;\n",
        "    padding-bottom: 0.2rem !important;\n",
        "    line-height: 1.2 !important;\n",
        "}\n",
        "\n",
        "/* Main app styling */\n",
        "[data-testid=\"stAppViewContainer\"] > section:first-of-type {\n",
        "    padding-top: 1rem !important;\n",
        "}\n",
        "[data-testid=\"stSidebarUserContent\"] {\n",
        "    padding-top: 1.5rem !important;\n",
        "}\n",
        "\n",
        "/* Recommendation Box Styling */\n",
        "[data-testid=\"stBorderedContainer\"] {\n",
        "    border: 1px solid #2c2f38 !important;\n",
        "    border-radius: 10px !important;\n",
        "    padding: 1rem !important;\n",
        "    margin-bottom: 1rem !important;\n",
        "}\n",
        "/* Title inside the box */\n",
        ".rec-title {\n",
        "    margin-top: 0rem !important;\n",
        "    padding-top: 0rem !important;\n",
        "    margin-bottom: 0.5rem;\n",
        "    color: #fafafa;\n",
        "    font-size: 1.25rem;\n",
        "    font-weight: 600;\n",
        "}\n",
        "/* Poster box */\n",
        ".poster-box {\n",
        "    width: 150px;\n",
        "    height: 210px;\n",
        "    background-size: cover;\n",
        "    background-position: center center;\n",
        "    border-radius: 8px;\n",
        "    border: 1px solid #444;\n",
        "    margin-bottom: 1.25rem;\n",
        "}\n",
        "/* Popover spacing */\n",
        "[data-testid=\"stPopoverBody\"] h3 {\n",
        "    margin-bottom: 0.25rem;\n",
        "}\n",
        "[data-testid=\"stPopoverBody\"] p {\n",
        "    margin-bottom: 0.75rem;\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "# --- 3. Title ---\n",
        "st.title(\"üé¨ Anime Recommendation System\")\n",
        "st.write(\"Created by Rahul Goyal. A deployable recommendation engine.\")\n",
        "\n",
        "# --- 4. Sidebar ---\n",
        "# --- SECTION 1: SEARCH ---\n",
        "st.sidebar.header(\"Find Recommendations\")\n",
        "\n",
        "# Create the list of searchable names\n",
        "all_names = list(anime['Name'].unique())\n",
        "english_names = list(anime[anime['English name'] != 'Unknown']['English name'].unique())\n",
        "search_options = sorted(list(set(all_names + english_names)))\n",
        "\n",
        "# Use selectbox instead of text_input\n",
        "anime_name = st.sidebar.selectbox(\n",
        "    \"Enter or Select an Anime:\",\n",
        "    options=search_options,\n",
        "    index=None,\n",
        "    placeholder=\"Type to search...\"\n",
        ")\n",
        "\n",
        "rec_type = st.sidebar.selectbox(\n",
        "    \"Choose a Recommendation Type:\",\n",
        "    (\n",
        "        \"Model-Based Similarity\",\n",
        "        \"Content (Plot) Similarity\",\n",
        "        \"Combined Model + Genre\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Popover Button (Standard style with arrow)\n",
        "with st.sidebar.popover(\"‚ìò What does this do?\"):\n",
        "    if rec_type == \"Model-Based Similarity\":\n",
        "        st.markdown(\"**Model-Based Similarity:**\")\n",
        "        st.write(\"Finds anime that other users rated in a similar way. This is a good 'if you liked this, you might also like...' feature based on the tastes of thousands of users.\")\n",
        "\n",
        "    elif rec_type == \"Content (Plot) Similarity\":\n",
        "        st.markdown(\"**Content (Plot) Similarity:**\")\n",
        "        st.write(\"Finds animes with similar plotlines or similar themes.\")\n",
        "\n",
        "    elif rec_type == \"Combined Model + Genre\":\n",
        "        st.markdown(\"**Combined Model + Genre:**\")\n",
        "        st.write(\"First finds anime with similar rating patterns, then filters that list to only show ones that also have good genre similarity with the input anime genres.\")\n",
        "\n",
        "\n",
        "show_input_details = st.sidebar.checkbox(\"Show details for input anime\", value=True)\n",
        "\n",
        "# --- NEW CHECKBOX ---\n",
        "remove_related = st.sidebar.checkbox(\"Remove related (prequels/sequels/spin-offs)\", value=False)\n",
        "with st.sidebar.popover(\"‚ÑπÔ∏è Info\"):\n",
        "    st.markdown(\"**Remove Related:**\")\n",
        "    st.write('''\n",
        "    We filter out major relations (Sequels/Prequels/Spin-offs/Side-stories) of the input anime from recommendations.\n",
        "    \\nNote: Loosely related animes listed under categories like 'Other' or 'Character' in related entries on MyAnimeList website will not be removed''')\n",
        "# --------------------\n",
        "\n",
        "genre_threshold = 0.5\n",
        "if rec_type == \"Combined Model + Genre\":\n",
        "    genre_threshold = st.sidebar.slider(\n",
        "        \"Min. Genre Similarity:\", 0.0, 1.0, 0.5, 0.05,\n",
        "        help=\"This is the minimum cosine similarity required with the input anime's genres.\"\n",
        "    )\n",
        "top_n_search = st.sidebar.slider(\"Number of recommendations:\", 5, 20, 10, key=\"search_slider\")\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.subheader(\"Filter Your Results (Optional)\")\n",
        "type_options = ['TV', 'Movie', 'OVA', 'Special', 'ONA']\n",
        "all_genres = sorted(genres_list)\n",
        "min_year = int(anime['Origin_year'].min())\n",
        "max_year = int(anime['Origin_year'].max())\n",
        "min_pop = int(anime['Popularity_adjusted'].min())\n",
        "max_pop = int(anime['Popularity_adjusted'].max())\n",
        "\n",
        "genres_preferred = st.sidebar.multiselect(\"Must include all of these genres:\", all_genres)\n",
        "type_preferred = st.sidebar.multiselect(\"Must be one of these types:\", type_options, default=type_options)\n",
        "min_anime_rating = st.sidebar.slider(\"Minimum average user rating:\", 0.0, 10.0, 0.0, 0.1)\n",
        "origin_year_range = st.sidebar.slider(\"Origin Year:\", min_year, max_year, (min_year, max_year))\n",
        "popularity_range = st.sidebar.slider(\"Popularity Rank (1 = Most Popular):\", min_pop, max_pop, (min_pop, max_pop))\n",
        "\n",
        "search_button = st.sidebar.button(\"Get Recommendations\", type=\"primary\")\n",
        "\n",
        "\n",
        "# --- SECTION 2: DISCOVER ---\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.header(\"Discover\")\n",
        "\n",
        "# Main action button\n",
        "discover_button = st.sidebar.button(\"Show 'Divided Opinion' Anime\", type=\"primary\")\n",
        "\n",
        "# Small info button on the next line\n",
        "with st.sidebar.popover(\"‚ÑπÔ∏è Info\"):\n",
        "    st.markdown(\"**Divided Opinion Anime:**\")\n",
        "    st.write(\"\"\"\n",
        "    These are polarizing anime. A similar number of users loved them (rating 8+) as hated them (rating < 5).\n",
        "\n",
        "    We'll show you a random selection from this pool for you to discover!\n",
        "    \"\"\")\n",
        "\n",
        "top_n_discover = st.sidebar.slider(\"Number of Animes:\", 5, 30, 10, key=\"discover_slider\")\n",
        "\n",
        "\n",
        "# --- 5. Store Filters ---\n",
        "user_filters = {\n",
        "    \"Genres_preferred\": genres_preferred if genres_preferred else None,\n",
        "    \"Type_preferred\": type_preferred if type_preferred else None,\n",
        "    \"min_anime_rating\": min_anime_rating,\n",
        "    \"Origin_year_range\": origin_year_range,\n",
        "    \"popularity_range\": popularity_range\n",
        "}\n",
        "\n",
        "\n",
        "# --- 6. Main Page Display Logic ---\n",
        "def display_recommendations(recommendations_df, is_input_anime=False):\n",
        "    \"\"\"Helper function to display results in a nice layout.\"\"\"\n",
        "\n",
        "    if is_input_anime:\n",
        "        row = recommendations_df.iloc[0]\n",
        "        st.markdown(f'<h3 class=\"rec-title\">Details for Input Anime: {row[\"Name\"]}</h3>', unsafe_allow_html=True)\n",
        "\n",
        "        with st.container(border=True):\n",
        "            col1, col2 = st.columns([1, 4])\n",
        "            with col1:\n",
        "                if row['image_url'] and row['image_url'] != \"NOT_FOUND\":\n",
        "                    image_style = f\"background-image: url('{row['image_url']}')\"\n",
        "                else:\n",
        "                    image_style = \"background-image: url('https://via.placeholder.com/150x210.png?text=No+Poster')\"\n",
        "                st.markdown(f'<div class=\"poster-box\" style=\"{image_style}\"></div>', unsafe_allow_html=True)\n",
        "            with col2:\n",
        "                st.write(f\"**Type:** {row['Type']}  |  **Year:** {row['Origin_year']}  |  **Avg. Rating:** {row['anime_avg_rating']:.2f} | **Popularity Rank:** {row['Popularity_adjusted']}\")\n",
        "                st.write(f\"**Genres:** {row['Genres_edited'].replace('|', ', ')}\")\n",
        "                if pd.notna(row['synopsis']):\n",
        "                    with st.expander(\"Show Synopsis\", expanded=False):\n",
        "                        st.write(row['synopsis'])\n",
        "\n",
        "    else:\n",
        "        for i, row in recommendations_df.reset_index(drop=True).iterrows():\n",
        "            with st.container(border=True):\n",
        "                st.markdown(f'<h3 class=\"rec-title\">{i + 1}. {row[\"Name\"]}</h3>', unsafe_allow_html=True)\n",
        "                col1, col2 = st.columns([1, 4])\n",
        "                with col1:\n",
        "                    if row['image_url'] and row['image_url'] != \"NOT_FOUND\":\n",
        "                        image_style = f\"background-image: url('{row['image_url']}')\"\n",
        "                    else:\n",
        "                        image_style = \"background-image: url('https://via.placeholder.com/150x210.png?text=No+Poster')\"\n",
        "                    st.markdown(f'<div class=\"poster-box\" style=\"{image_style}\"></div>', unsafe_allow_html=True)\n",
        "                with col2:\n",
        "                    st.write(f\"**Type:** {row['Type']}  |  **Year:** {row['Origin_year']}  |  **Avg. Rating:** {row['anime_avg_rating']:.2f} | **Popularity Rank:** {row['Popularity_adjusted']}\")\n",
        "                    st.write(f\"**Genres:** {row['Genres_edited'].replace('|', ', ')}\")\n",
        "                    if pd.notna(row['synopsis']):\n",
        "                        with st.expander(\"Show Synopsis\"):\n",
        "                            st.write(row['synopsis'])\n",
        "\n",
        "# --- Updated logic for which button was pressed ---\n",
        "if search_button:\n",
        "    if anime_name:\n",
        "        anime_details = get_anime_details(anime_name, anime, anime_agg)\n",
        "\n",
        "        if anime_details is None:\n",
        "            st.error(\"Anime is not found, please check the name and try again\")\n",
        "        else:\n",
        "            if show_input_details:\n",
        "                display_recommendations(anime_details, is_input_anime=True)\n",
        "                st.markdown(\"---\")\n",
        "\n",
        "            with st.spinner('Searching for the best recommendations...'):\n",
        "                recommendations = get_recommendations_by_name(\n",
        "                    anime_name,\n",
        "                    rec_type,\n",
        "                    top_n_search,\n",
        "                    genre_threshold=genre_threshold,\n",
        "                    remove_related=remove_related, # <-- Pass the new argument\n",
        "                    **user_filters\n",
        "                )\n",
        "\n",
        "                if recommendations is not None and not recommendations.empty:\n",
        "                    st.success(f\"Here are the top {len(recommendations)} recommendations for '{anime_name}':\")\n",
        "                    display_recommendations(recommendations, is_input_anime=False)\n",
        "                else:\n",
        "                    st.error(f\"No recommendations found for '{anime_name}' with the selected filters. Try broadening your search!\")\n",
        "    else:\n",
        "        st.warning(\"Please enter an anime name.\")\n",
        "\n",
        "elif discover_button:\n",
        "    with st.spinner(\"Finding controversial anime...\"):\n",
        "        divided_animes = get_discover_animes(top_n=top_n_discover)\n",
        "        if not divided_animes.empty:\n",
        "            st.success(f\"ü§î Here are {len(divided_animes)} 'Divided Opinion' animes for you:\")\n",
        "            display_recommendations(divided_animes, is_input_anime=False)\n",
        "        else:\n",
        "            st.error(\"No 'Divided Opinion' animes found.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Choose an option from the sidebar to get started!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBTeQpt4Fhk5",
        "outputId": "6163fc2b-4176-4b6e-b8df-91bdb217a717"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/app.py\n"
          ]
        }
      ]
    }
  ]
}